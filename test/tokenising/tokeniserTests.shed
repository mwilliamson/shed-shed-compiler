package shed.compiler.tokenising;

import hat.TestCase;
import duck.assertThat;
import duck.isList;
import duck.equalTo;

import shed.compiler.StringSource;
import shed.compiler.tokenising.tokens.Token;
import shed.compiler.tokenising.tokens;
import shed.compiler.tokenising.Tokeniser;

def assertIsTokenisedTo fun(input: StringSource, expectedTokens: List[Token]) => do {
    val tokeniser = Tokeniser();
    val actualTokens = tokeniser.tokenise(input);
    val expectedMatchers = expectedTokens.map(equalTo[Token]);
    return assertThat[List[Token]](actualTokens, isList[Token](expectedMatchers));
};
    

public val tokeniserTests = listOf[TestCase](
    TestCase("empty string => end token", fun() => do {
        val source = stringSource("");
        return assertIsTokenisedTo(
            source,
            listOf[Token](tokens.end(source.range(0, 0)))
        );
    }),
    
    TestCase("alphanumeric string => identifier", fun() => do {
        val source = stringSource("blah");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.identifier("blah", source.range(0, 4)),
                tokens.end(source.range(4, 4))
            )
        );
    }),
    
    TestCase("whitespace string => whitespace token", fun() => do {
        val source = stringSource(" \t\n");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.whitespace(" \t\n", source.range(0, 3)),
                tokens.end(source.range(3, 3))
            )
        );
    }),
    
    TestCase("identifiers separated by whitespace", fun() => do {
        val source = stringSource("first   second\t third");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.identifier("first", source.range(0, 5)),
                tokens.whitespace("   ", source.range(5, 8)),
                tokens.identifier("second", source.range(8, 14)),
                tokens.whitespace("\t ", source.range(14, 16)),
                tokens.identifier("third", source.range(16, 21)),
                tokens.end(source.range(21, 21))
            )
        );
    }),
    
    TestCase("keywords are recognised", fun() => do {
        val source = stringSource("fun");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.keyword("fun", source.range(0, 3)),
                tokens.end(source.range(3, 3))
            )
        );
    }),
    
    TestCase("single character symbol is tokenised", fun() => do {
        val source = stringSource("(");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.symbol("(", source.range(0, 1)),
                tokens.end(source.range(1, 1))
            )
        );
    }),
    
    TestCase("adjacent symbols are tokenised separately", fun() => do {
        val source = stringSource("()");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.symbol("(", source.range(0, 1)),
                tokens.symbol(")", source.range(1, 2)),
                tokens.end(source.range(2, 2))
            )
        );
    })
);
    
def stringSource fun(string: String) =>
    StringSource(string, "raw string", 0);
    
