package shed.compiler.tokenising;

import hat.TestCase;
import duck.assertThat;
import duck.isList;
import duck.equalTo;

import shed.compiler.StringSource;
import shed.compiler.tokenising.tokens.Token;
import shed.compiler.tokenising.tokens;
import shed.compiler.tokenising.Tokeniser;

def assertIsTokenisedTo fun(input: StringSource, expectedTokens: List[Token]) => do {
    val tokeniser = Tokeniser();
    val actualTokens = tokeniser.tokenise(input);
    val expectedMatchers = expectedTokens.map(equalTo[Token]);
    return assertThat[List[Token]](actualTokens, isList[Token](expectedMatchers));
};
    

public val tokeniserTests = listOf[TestCase](
    TestCase("empty string => end token", fun() => do {
        val source = stringSource("");
        return assertIsTokenisedTo(
            source,
            listOf[Token](tokens.end(source.range(0, 0)))
        );
    }),
    
    TestCase("alphanumeric string => identifier", fun() => do {
        val source = stringSource("blah");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.identifier("blah", source.range(0, 4)),
                tokens.end(source.range(4, 4))
            )
        );
    }),
    
    TestCase("whitespace string => whitespace token", fun() => do {
        val source = stringSource(" \t\n");
        return assertIsTokenisedTo(
            source,
            listOf[Token](
                tokens.whitespace(" \t\n", source.range(0, 3)),
                tokens.end(source.range(3, 3))
            )
        );
    })
);
    
def stringSource fun(string: String) =>
    StringSource(string, "raw string");
    
