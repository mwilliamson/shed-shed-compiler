module shed/compiler/parsing/parser;

members::
    Parser

import lop/token.Token;
import lop/results.Success;

import shed/compiler/tokenising/tokeniser.Tokeniser;
import shed/compiler/tokenising/tokenFilter.filterTokens;
import shed/compiler/nodes;
import shed/compiler/results;
import shed/compiler/parsing/expressions.expressionRules;
import shed/compiler/parsing/statements.statementRule;
import shed/compiler/parsing/modules.moduleRule;

def Parser class() => ::
    members::
        parse,
        parseWithoutEndToken,
        parseModule
    
    val tokeniser = Tokeniser();
        
    def parse fun(rule: Rule, input: StringSource) =>
        tokenise(input)
            .map(rule)
            .valueOrElse()
    
    def filteredParse fun(predicate: Function[Token, Boolean]) =>
        fun(rule: Rule, input: StringSource) =>
            tokenise(input)
                .map(fun(tokens: List[Token]) => tokens.filter(predicate))
                .map(rule)
                .valueOrElse()
    
    val parseWithoutEndToken =
        filteredParse(fun(token: Token) =>
            not(token.name().equals("end"))
        );
    
    def parseModule fun(input: StringSource) => do::
        val parseResult = parse(parseRule(), input);
        return if parseResult.isSuccess() then
            results.success(parseResult.value())
        else
            results.failure(listOf(
                "Parse failed: "
                    .concat(parseResult.message().toString())
            ));

    def parseRule fun() => do::
        // HACK TODO FIXME: need a better way of doing indirect mutual recursion
        val optionalMembers = lazyFunction(fun() =>
            expressionRules(statement).optionalMembers()
        );
        
        val expression = lazyFunction(fun() =>
            expressionRules(statement).expression()
        );
        
        val statement = lazyFunction(fun() =>
            statementRule(expression)
        );
        
        return moduleRule(statement, optionalMembers);
    
    def tokenise fun(input: StringSource) =>
        filterTokens(tokeniser.tokenise(input));
