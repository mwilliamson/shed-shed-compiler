module shed.compiler.parsing.parser;

members {
    Parser
}

import lazySequences;

import lop.token.Token;
import shed.compiler.tokenising.tokeniser.Tokeniser;
import lop.results.Success;
import shed.compiler.nodes;

def Parser class() => {
    members {
        parse,
        parseWithoutEndToken
    }
    
    val tokeniser = Tokeniser();
    
    def filteredParse fun(predicate: Function[Token, Boolean]) =>
        fun(rule: Rule, input: StringSource) =>
            rule(lazySequences.filter(predicate, tokenise(input)))
        
    // TODO: combine filtering
    val parse = 
        filteredParse(fun(token: Token) => not(or(
            token.name().equals("whitespace"),
            token.name().equals("comment")
        )));
    
    val parseWithoutEndToken =
        filteredParse(fun(token: Token) =>
            not(or(
                token.name().equals("whitespace"),
                token.name().equals("end"),
                token.name().equals("comment")
            )));
    
    def tokenise fun(input: StringSource) =>
        tokeniser.tokenise(input).toSequence();
}
