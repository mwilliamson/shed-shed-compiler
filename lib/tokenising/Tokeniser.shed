module shed.compiler.tokenising.tokeniser;

members {
    Tokeniser
}

import json;
import regex;
import options.none;
import options.some;
import sequences;
import lazySequenceables.map;
import lists.sequenceToList;
import sets;
import strings;

import shed.compiler.tokenising.tokens;
import lop.token.Token;
import lop.sources.StringSource;

def Tokeniser class() => {
    members {
        tokenise
    }
    
    val keywords = sets.fromList[String](listOf(
        "true", "false", "return", "package", "import", "val", "var", "public",
        "object", "class", "interface", "if", "else", "while", "fun", "for",
        "def", "then", "do", "members", "module"
    ));
    
    val symbols = sets.fromList[String](listOf(
        "=>", "->", "<:", "`", "Â¬", "!", "Â£", "$", "%", "^", "&", "*", "(", ")", "-",
        "=", "+", "[", "]", "{", "}", ";", ":", "'", "@", "#", "~", "<",
        ">", ",", ".", "/", "?", "\\", "|"
    ));
    
    def tokenise fun(input: StringSource) =>
        sequenceToList[Token](tokeniseString(input));
    
    def tokeniseString fun(input: StringSource) => do {
        val length = input.asString().length();
        return if length.greaterThan(0) then do {
            val nextToken = readNextToken(input);
            return sequences.lazyCons[Token](
                nextToken.token(),
                fun() => tokeniseString(nextToken.rest())
            );
        } else
            sequences.singleton[Token](tokens.end(input.range(0, 0)));
    };
        
    def readNextToken fun(input: StringSource) =>
        listOf(readLineComment, readWhitespace, readString, readSymbol, readNumber, readIdentifier)
            .foldLeft(none, fun(result: Option[NextToken], reader: Function[StringSource, Option[NextToken]]) =>
                result.orElse[NextToken](fun() => reader(input))
            )
            .valueOrElse[NextToken](fun() => NextToken(
                tokens.symbol(input.asString().substring(0, 1), input.range(0, 1)),
                input.sliceFrom(1)
            ));
    
    def regexReader fun(regex: Regex, tokenBuilder: function[String, StringSource, Token]) =>
        fun(input: StringSource) => do {
            val string = input.asString();
            
            return regex.exec(string).map[NextToken](fun(regexResult: RegexResult) => do {
                val value = regexResult.capture(1);
                return NextToken(
                    tokenBuilder(value, input.range(0, value.length())),
                    input.sliceFrom(value.length())
                );
            });
        };
        
    def alphanumericToken fun(value: String, source: StringSource) => do {
        val tokenBuilder = if keywords.contains(value) then
            tokens.keyword
        else
            tokens.identifier;
        return tokenBuilder(value, source);
    }
    
    def capture fun(value: String) =>
        regex.create("^(".concat(value).concat(")"));
    
    val readIdentifier = regexReader(capture("[a-zA-Z_][a-zA-Z0-9_]*"), alphanumericToken);
    val readWhitespace = regexReader(capture("\\s+"), tokens.whitespace);
    val symbolRegex = capture(strings.join("|", map[String, String](regex.escape, symbols)));
    val readSymbol = regexReader(symbolRegex, tokens.symbol);
    val readNumber = regexReader(capture("[0-9]+"), tokens.number);
    val readLineComment = regexReader(capture("//.*"), tokens.comment);
    
    def createStringToken fun(value: String, source: StringSource) =>
        // HACK: should parse the string manually to avoid potential errors
        tokens.string(json.parseString(value), source);
    
    val readString = regexReader(capture("\"[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\""), createStringToken);
    
    def NextToken class(token: Token, rest: StringSource) => {
        members {
            token fun() => token,
            rest fun() => rest
        }
    }
};
