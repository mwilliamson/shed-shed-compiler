package shed.compiler.tokenising;

import regex;
import options.none;
import options.some;
import sequences;
import lazySequenceables.map;
import lists.sequenceToList;
import sets;
import strings;

import shed.compiler.tokenising.tokens;
import shed.compiler.tokenising.tokens.Token;
import shed.compiler.StringSource;

public def Tokeniser class() => {
    val keywords = sets.fromList[String](listOf(
        "true", "false", "return", "package", "import", "val", "var", "public",
        "object", "class", "interface", "if", "else", "while", "fun", "for",
        "def", "then", "do"
    ));
    
    val symbols = sets.fromList[String](listOf(
        "=>", "->", "<:", "`", "¬", "!", "£", "$", "%", "^", "&", "*", "(", ")", "-",
        "=", "+", "[", "]", "{", "}", ";", ":", "'", "@", "#", "~", "<",
        ">", ",", ".", "/", "?", "\\", "|"
    ));
    
    public def tokenise fun(input: StringSource) =>
        sequenceToList[Token](tokeniseString(input));
    
    def tokeniseString fun(input: StringSource) => do {
        val length = input.asString().length();
        return if length.greaterThan(0) then do {
            val nextToken = readNextToken(input);
            return sequences.lazyCons[Token](
                nextToken.token(),
                fun() => tokeniseString(nextToken.rest())
            );
        } else
            sequences.singleton[Token](tokens.end(input.range(0, 0)));
    };
        
    def readNextToken fun(input: StringSource) =>
        listOf(readLineComment, readWhitespace, readString, readSymbol, readNumber, readIdentifier)
            .foldLeft(none, fun(result: Option[NextToken], reader: Function[StringSource, Option[NextToken]]) =>
                result.orElse[NextToken](fun() => reader(input))
            )
            .valueOrElse[NextToken](fun() => NextToken(
                tokens.symbol(input.asString().substring(0, 1), input.range(0, 1)),
                input.slice(1)
            ));
    
    def regexReader fun(regex: Regex, tokenBuilder: function[String, StringRange, Token]) =>
        fun(input: StringSource) => do {
            val string = input.asString();
            
            return regex.exec(string).map[NextToken](fun(regexResult: RegexResult) => do {
                val value = regexResult.capture(1);
                return NextToken(
                    tokenBuilder(value, input.range(0, value.length())),
                    input.slice(value.length())
                );
            });
        };
        
    def alphanumericToken fun(value: String, source: StringRange) => do {
        val tokenBuilder = if keywords.contains(value) then
            tokens.keyword
        else
            tokens.identifier;
        return tokenBuilder(value, source);
    }
    
    def capture fun(value: String) =>
        regex.create("^(".concat(value).concat(")"));
    
    val readIdentifier = regexReader(capture("[a-zA-Z][a-zA-Z0-9]*"), alphanumericToken);
    val readWhitespace = regexReader(capture("\\s+"), tokens.whitespace);
    val symbolRegex = capture(strings.join("|", map[String, String](regex.escape, symbols)));
    val readSymbol = regexReader(symbolRegex, tokens.symbol);
    val readNumber = regexReader(capture("0"), tokens.number);
    val readLineComment = regexReader(capture("//.*"), tokens.comment);
    
    def createStringToken fun(value: String, source: StringRange) =>
        tokens.string(value.substring(1, value.length().subtract(1)), source);
    
    val readString = regexReader(capture("\"[^\"]*\""), createStringToken);
    
    def NextToken class(myToken: Token, myRest: StringSource) => {
        public def token fun() => myToken;
        public def rest fun() => myRest;
    }
};
