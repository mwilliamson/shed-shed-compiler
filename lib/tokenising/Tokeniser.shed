package shed.compiler.tokenising;

import regex;

import shed.compiler.tokenising.tokens;
import shed.compiler.tokenising.tokens.Token;

public def Tokeniser class() => {
    public def tokenise fun(input: StringSource) => do {
        val length = input.asString().length();
        return if length.greaterThan(0) then
            listOf[Token](
                stringToToken(input),
                tokens.end(input.range(length, length))
            )
        else
            listOf[Token](tokens.end(input.range(0, 0)));
    };
    
    def stringToToken fun(input: StringSource) =>
        if isWhitespace(input.asString()) then
            tokens.whitespace(input.asString(), input.range(0, input.asString().length()))
        else
            tokens.identifier(input.asString(), input.range(0, input.asString().length()))
            
    def isWhitespace fun(string: String) =>
        regex.create("\\s+").test(string)
};
